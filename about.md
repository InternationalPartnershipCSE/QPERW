---
layout: default
title: About
permalink: /about/
---
<img src="https://www.researchgate.net/profile/Richard_Hake/publication/251636536/figure/fig1/AS:669000468017154@1536513549292/From-Hake-1998a-Gain-vs-Pretest-score-on-the-conceptual-Mechanics-Diagnostic.ppm">

## Background
Quantitative methods in Physics Education Research has been used to make strong arguments for instructional and curricula change, examining differences in student populations, assessing student attitudes towards physics in different contexts, and recommending other large scale reforms. This work includes not only using statistical methods that are commonly used in social science but also inventing new metrics like normalized gain (Hake, 1998) or visualizing longitudinal patterns in educational data (Bendinelli and Marder, 2012). Papers in PER that use regression modeling typically explore data only via logistic or linear regressions and do not use cox style regressions or modern machine learning models (with few exceptions, e.g. Young et al., 2019).

Recent efforts in PER have pushed the envelope of quantitative methods, theory, and data available to PER scientists. Last year, Physical Review: Physics Education Research published a focused collection titled “Quantitative Methods in PER: A Critical Examination” (Knaub, Aiken, Caballero, 2019). This collection examined topics such as handling network data(Dou and Zwolak, 2019), new item response theory methods (Zabriskie and Stewart, 2019; Planinic, 2019), and dealing with missing data (Nissen, Donatello, Van Dusen, 2019). PER has begun collecting large scale data sets that include registrar data (Aiken, Henderson, Caballero, 2019), attitudinal surveys (Wilcox et al., 2016), and concept inventory data (Nissen et al., 2018).

Much of recent literature across social science and statistics has called for a large change in the types of models used to predict social systems, the theory and frameworks that motivates the choice of models, and the evaluation methods used to demonstrate prediction. Hoffman, Sharma, and Watts (2017) recommends that “current practices for evaluating predictions must be better standardized”, arguing that current methods in social science focus too much on explanation and too little on prediction. Hoffman, Sharma, and Watts (2017) recognizes that there is a fear that complex models may lose interpretability but points to many innovations in recent literature that overcome a loss of interpretability in complex models. Historically this conversation has been going on in statistics for many years. Breiman (2001) identified two separate cultures that use statistical models: one culture is grounded in the theory that data are generated from stochastic models, the other culture is grounded in the theory that the model that generates the data is unknown. Breiman (2001) argues that by assuming that data is generated from stochastic models, and thus only linear models such as ordinary least squares regression or multi-level regressions can be used, researchers artificially restrict themselves from a wide variety of models that may produce better understanding of the data.